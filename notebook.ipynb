{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#900C3F;\">Census Income Data Analysis</span>\n",
    "\n",
    "### <span style=\"color:#5E5C5B;\">In this notebook, using census income data, we construct various classification models, and implement them above on the data, and analyze the performance of each classification model.</span> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#900C3F;\">Steps taken:</span> \n",
    "\n",
    "1. **Setting up the environment.**\n",
    "2. **Preparing Data for Analysis.**\n",
    "3. **Implementation and Model evaluation.**\n",
    "4. **Comparing the performance of classification models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#900C3F;\">1. Setting up the environment</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing requried libraries**\n",
    "\n",
    "Mainly used:\n",
    "- **Apache spark** in this project, so we import the some libraries required, **pyspark** for starting a **spark session.** \n",
    "- **pandas** for data analysis and manipulation.\n",
    "- **sklearn** for statistical modeling including classification.\n",
    "- **plotly** for visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a **spark session** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#900C3F;\">2. Preparing Data for Analysis</span>\n",
    "\n",
    "**Preparing** dataset taken from http://archive.ics.uci.edu/ml/index.php for the analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the file with spark session's method read.csv, converting the csv file to a dataframe\n",
    "def readCSVtoDF(csvData):\n",
    "    return spark.read.csv(csvData)\n",
    "\n",
    "rawData = 'Data/adult.data'\n",
    "\n",
    "CensusDF = readCSVtoDF(rawData)\n",
    "\n",
    "AttributeList = CensusDF.columns\n",
    "\n",
    "Attributes = {'_c0':'age', '_c1':'workclass', '_c2':'fnlwg', '_c3':'education', '_c4':'education_number', \n",
    "              '_c5':'marital_status', '_c6':'occupation', '_c7':'relationship', '_c8':'race', '_c9':'sex', \n",
    "              '_c10':'capital_gain', '_c11':'capital_loss', '_c12':'hours_per_week', '_c13':'native_country', \n",
    "              '_c14':'income'}\n",
    "\n",
    "categorical_features = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex',\n",
    "                       'native_country', 'income']\n",
    "numerical_features = ['age', 'fnlwg', 'education_number', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
    "\n",
    "#changing the name of attributes with original attribute names\n",
    "for col in AttributeList:\n",
    "    CensusDF = CensusDF.withColumnRenamed(col, Attributes[col])\n",
    "    \n",
    "#converting dataframe to Pandas Dataframe\n",
    "def convertDFtoPandas(df):\n",
    "    return df.select('*').toPandas()\n",
    "data = convertDFtoPandas(CensusDF)\n",
    "dataUncleaned = convertDFtoPandas(CensusDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span>Dataset</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Census Income data** which is extracted by Barry Becker from the 1994 Census database has **multivarient dataset characteristics** and has two types of **attribute charactristics** which are Integer and Categorical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of instances and Number of attributes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uncleaned dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **Tree Decision** classifiers, we use uncleaned dataset and there are some missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataUncleaned[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing data** for categorical features which are listed above has some steps. First, we make **ordinal encoding** for workclass, marital_status and occupation columns. Second, we replaced **missing values** with the most frequent values. Third, we **scaled** the numerical data which is listed above. Fourth, we **cleaned** the data. We make a **normalization data process** by using **sklearn.preprocessing** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### prepocessing data for categorical features #######\n",
    "#workclass\n",
    "encoder_workclass = OrdinalEncoder()\n",
    "data.workclass = encoder_workclass.fit_transform(data.workclass.values.reshape(-1, 1))\n",
    "data.loc[data['workclass'] == 8, 'workclass'] = data['workclass'].mode()\n",
    "\n",
    "#marital_status\n",
    "encoder_marital_status = OrdinalEncoder()\n",
    "data.marital_status = encoder_marital_status.fit_transform(data.marital_status.values.reshape(-1, 1))\n",
    "data.loc[data['marital_status'] == 7, 'marital_status'] = data['marital_status'].mode()\n",
    "\n",
    "#occupation\n",
    "data.loc[data['occupation'].isnull(), 'occupation'] = data['occupation'].mode()\n",
    "encoder_occupation= OrdinalEncoder()\n",
    "data.occupation= encoder_occupation.fit_transform(data.occupation.values.reshape(-1,1))\n",
    "data.loc[data['occupation']== 14, 'occupation'] = data['occupation'].mode()\n",
    "\n",
    "#relationship\n",
    "encoder_relationship = OrdinalEncoder()\n",
    "data.relationship = encoder_relationship.fit_transform(data.relationship.values.reshape(-1, 1))\n",
    "#no null in this one\n",
    "\n",
    "#race\n",
    "encoder_race = OrdinalEncoder()\n",
    "data.race = encoder_race.fit_transform(data.race.values.reshape(-1, 1))\n",
    "#no null in this one\n",
    "\n",
    "#sex\n",
    "encoder_sex = OrdinalEncoder()\n",
    "data.sex = encoder_sex.fit_transform(data.sex.values.reshape(-1, 1))\n",
    "#no null in this one\n",
    "\n",
    "\n",
    "#native_country\n",
    "encoder_native_country= OrdinalEncoder()\n",
    "data.native_country= encoder_native_country.fit_transform(data.native_country.values.reshape(-1,1))\n",
    "data.loc[data['native_country']== 41, 'native_country'] = data['native_country'].mode()\n",
    "\n",
    "#income\n",
    "encoder_income = OrdinalEncoder()\n",
    "data.income= encoder_income.fit_transform(data.income.values.reshape(-1, 1))\n",
    "#no null in this one\n",
    "\n",
    "#education--- should be changed later\n",
    "\n",
    "encoder_education= OrdinalEncoder()\n",
    "data.education= encoder_education.fit_transform(data.education.values.reshape(-1, 1))\n",
    "#no null in this one\n",
    "\n",
    "\n",
    "age_scaler = MinMaxScaler(feature_range=(0,1))     #ages to be between o and 1 \n",
    "data.age =age_scaler.fit_transform(data.age.values.reshape(-1, 1))\n",
    "\n",
    "fnlwg_scaler = MinMaxScaler(feature_range=(0,1))    \n",
    "data.fnlwg =fnlwg_scaler.fit_transform(data.fnlwg.values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "education_number_scaler = MinMaxScaler(feature_range=(0,1))    \n",
    "data.education_number=education_number_scaler.fit_transform(data.education_number.values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "diff1 =int(max(data['capital_gain']))-int(min(data['capital_gain']))\n",
    "data['capital_gain'] = data['capital_gain'].apply(lambda x: int(x)/diff1)\n",
    "\n",
    "\n",
    "diff2 =int(max(data['capital_loss']))-int(min(data['capital_loss']))\n",
    "data['capital_loss'] = data['capital_loss'].apply(lambda x: int(x)/diff2)\n",
    "\n",
    "\n",
    "diff3 =int(max(data['hours_per_week']))-int(min(data['hours_per_week']))\n",
    "data['hours_per_week'] = data['hours_per_week'].apply(lambda x: int(x)/diff3)\n",
    "\n",
    "\n",
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "data = clean_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#900C3F;\">3. Implementation and Model evaluation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use different methods for 6 classifiers; namely, **Desicion tree using gain ratio, Desicion tree using gini index, Naïve Bayes, Artificial neural networks with 1 hidden layer, Artificial neural networks with 2 hidden layers, Support vector machines**. While implementing these 6 algorithms on our data, we mainly use **sklearn** library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiersList = ['Decision tree using gain ratio', 'Decision tree using gini index', 'Naïve Bayes',\n",
    "                  'Artificial neural networks with 1 hidden layer', 'Artificial neural networks with 2 hidden layers',\n",
    "                  'Support vector machines']\n",
    "accuracyComparison_holdout = []  #all the hold out accuracies for each classifier will be appended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span>Decision tree using gain ratio</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_gainRatio = []\n",
    "usedMethods_gainRatio = ['Holdout Method', 'Bagging Ensemble Method']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the entropy of each column\n",
    "def entropy(col): \n",
    "    values,counts = np.unique(col,return_counts = True) \n",
    "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(values))])\n",
    "    return entropy\n",
    "\n",
    "#calculating the gain\n",
    "def Gain(dataframe,split_attribute,target_name=\"income\"):   #it will always be income\n",
    "  \n",
    "    total_entropy = entropy(dataframe[target_name])    #Calculating the entropy of target name\n",
    "    Weighted_Entropy =  entropy(dataframe[split_attribute])\n",
    "    \n",
    "    #Calculate the information gain\n",
    "    Information_Gain = total_entropy - Weighted_Entropy\n",
    "    return Information_Gain  \n",
    "\n",
    "#ID3 algorithm for decision tree using gain \n",
    "\n",
    "def ID3(data,originaldata,features,target_attribute_name=\"income\",parent_node = None):\n",
    "    \n",
    "    #if all the target column values are same, return that value! no need to grow the tree\n",
    "    \n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:            \n",
    "    \n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    "      \n",
    "   \n",
    "    \n",
    "    elif len(features) ==0:               # when the feature space is empty\n",
    "        return parent_node\n",
    "    \n",
    "    \n",
    "    #growing the tree\n",
    "    \n",
    "    else:\n",
    "      \n",
    "        #the default value for the parent node is that value that appears the most in that specific feature.\n",
    "        parent_node = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n",
    "        \n",
    "        #calculating Gain for each feature\n",
    "        item_values = [Gain(data,feature,target_attribute_name) for feature in features]\n",
    "        \n",
    "        #choosing the highest feature\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        best_feature = features[best_feature_index] #to be assigned as the root \n",
    "        \n",
    "        #Tree structure\n",
    "        \n",
    "        tree = {best_feature:{}}\n",
    "        \n",
    "        \n",
    "        # remove the best feature\n",
    "        features = [i for i in features if i != best_feature]\n",
    "        \n",
    "        #Grow a branch under the root node for each possible value of the root node feature\n",
    "        \n",
    "        for value in np.unique(data[best_feature]):\n",
    "            value = value\n",
    "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n",
    "            sub_data = data.where(data[best_feature] == value).dropna()\n",
    "            \n",
    "            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
    "            subtree = ID3(sub_data,originaldata,features,target_attribute_name,parent_node)\n",
    "            \n",
    "            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
    "            tree[best_feature][value] = subtree\n",
    "            \n",
    "        return(tree)    \n",
    "    \n",
    "# for future predictions a query of specified \n",
    "#features will be given and prediction function will go through the tree to find the result    \n",
    "def predict(query,tree,default = 1): \n",
    "    \n",
    "    for key in list(query.keys()): \n",
    "        if key in list(tree.keys()):\n",
    "            try:\n",
    "                result = tree[key][query[key]] \n",
    "            except:\n",
    "                return default\n",
    "            result = tree[key][query[key]]\n",
    "            \n",
    "            if isinstance(result,dict):\n",
    "                return predict(query,result)\n",
    "\n",
    "            else:\n",
    "                return result\n",
    "\n",
    "def test(data,tree):\n",
    "    #Create new query instances by simply removing the target feature column from the original dataset and \n",
    "    #convert it to a dictionary\n",
    "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
    "    \n",
    "    #Create a empty DataFrame in whose columns the prediction of the tree are stored\n",
    "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
    "    predicted.sort_index(inplace=True)\n",
    "    \n",
    "    #Calculate the prediction accuracy\n",
    "    for i in range(len(data)):\n",
    "        predicted.loc[i,\"predicted\"] = predict(queries[i],tree) \n",
    "        \n",
    "\n",
    "    columns = ['income']\n",
    "    testdf=pd.DataFrame(data[\"income\"], columns=columns)    \n",
    "    accuracy = (np.sum(predicted[\"predicted\"] == testdf[\"income\"])/len(data)*100)\n",
    "    accuracyComparison_holdout.append(accuracy)\n",
    "    accuracy_gainRatio.append(accuracy) \n",
    "    print()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Holdout Method</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#holdout\n",
    "def holdout_entropy(dataset):\n",
    "    training_data,testing_data = train_test_split(dataset,test_size=0.2, random_state=100)\n",
    "    training_data_hold = training_data.reset_index(drop=True)\n",
    "    testing_data_hold = testing_data.reset_index(drop=True)\n",
    "    tree = ID3(training_data_hold,training_data_hold,training_data_hold.columns[:-1])\n",
    "    return test(testing_data_hold,tree)\n",
    "resultGain = holdout_entropy(dataUncleaned)\n",
    "our_error_ID3 = 100 - resultGain\n",
    "resultGain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Bagging Ensemble Method</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bagging\n",
    "def bagging_entropy(dataset,n):\n",
    "    result=0\n",
    "    for i in range(n):\n",
    "        rnge =int(len(dataset)/n)\n",
    "        start = i*rnge\n",
    "        end= start+ rnge\n",
    "        dataset = dataset[start:end]\n",
    "        training_data,testing_data = train_test_split(dataset,test_size=0.2, random_state=n*100)\n",
    "        training_data= training_data.reset_index(drop=True)\n",
    "        testing_data= testing_data.reset_index(drop=True)\n",
    "        tree = ID3(training_data,training_data,training_data.columns[:-1])\n",
    "        accuracy_for_each_bag = test(testing_data,tree)\n",
    "        result+=accuracy_for_each_bag\n",
    "        accuracy = result/n\n",
    "        accuracy_gainRatio.append(accuracy)\n",
    "    print(result/n)\n",
    "bagging_entropy(dataUncleaned,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Which method gives the most accurate value for Tree Decision using gain ratio Classifier?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracyPlot_gainRatio(methodList, accuracyList):\n",
    "    fig = go.Figure(data=[go.Pie(labels=methodList, values=accuracyList)])\n",
    "    return fig.update_traces(hoverinfo='label+value', textinfo='percent', marker=dict(colors= ['darkseagreen', 'darkgreen']))\n",
    "accuracyPlot_gainRatio(usedMethods_gainRatio, accuracy_gainRatio).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span>Decision tree using gini index</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_giniIndex = []\n",
    "usedMethods_giniIndex = ['Holdout Method', 'Bagging Ensemble Method']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## decision tree by gini \n",
    "\n",
    "def gini(col): \n",
    "    values,counts = np.unique(col,return_counts = True) \n",
    "    gini = np.sum([(1-(counts[i]/np.sum(counts))*(counts[i]/np.sum(counts))) for i in range(len(values))])\n",
    "    return gini\n",
    "\n",
    "#calculating the gini difference\n",
    "def ginidiff(dataframe,split_attribute,target_name=\"income\"):   #it will always be income\n",
    "  \n",
    "    total_gini = gini(dataframe[target_name])    #Calculating the entropy of target name\n",
    "    Weighted_gini =  gini(dataframe[split_attribute])\n",
    "    \n",
    "    #Calculate the information gain\n",
    "    gini_diff = total_gini - Weighted_gini\n",
    "    return gini_diff  \n",
    "\n",
    "#c45 algorithm for decision tree using gain \n",
    "def C45(data,originaldata,features,target_attribute_name=\"income\",parent_node = None):\n",
    "    \n",
    "    #if all the target column values are same, return that value! no need to grow the tree\n",
    "    \n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:            \n",
    "    \n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    "      \n",
    "   \n",
    "    \n",
    "    elif len(features) ==0:               # when the feature space is empty\n",
    "        return parent_node\n",
    "    \n",
    "    \n",
    "    #growing the tree\n",
    "    \n",
    "    else:\n",
    "      \n",
    "        #the default value for the parent node is that value that appears the most in that specific feature.\n",
    "        parent_node = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n",
    "        \n",
    "        #calculating Gain for each feature\n",
    "        item_values = [ginidiff(data,feature,target_attribute_name) for feature in features]\n",
    "        \n",
    "        #choosing the highest feature\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        best_feature = features[best_feature_index] #to be assigned as the root \n",
    "        \n",
    "        #Tree structure\n",
    "        \n",
    "        tree = {best_feature:{}}\n",
    "        \n",
    "        \n",
    "        # remove the best feature\n",
    "        features = [i for i in features if i != best_feature]\n",
    "        \n",
    "        #Grow a branch under the root node for each possible value of the root node feature\n",
    "        \n",
    "        for value in np.unique(data[best_feature]):\n",
    "            value = value\n",
    "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n",
    "            sub_data = data.where(data[best_feature] == value).dropna()\n",
    "            \n",
    "            #Call the C45 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
    "            subtree = C45(sub_data,originaldata,features,target_attribute_name,parent_node)\n",
    "            \n",
    "            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
    "            tree[best_feature][value] = subtree\n",
    "            \n",
    "        return(tree)    \n",
    "\n",
    "# for future predictions a query of specified \n",
    "#features will be given and prediction function will go through the tree to find the result    \n",
    "def predict_gini(query,tree,default = 1): \n",
    "    \n",
    "    for key in list(query.keys()): \n",
    "        if key in list(tree.keys()):\n",
    "            try:\n",
    "                result = tree[key][query[key]] \n",
    "            except:\n",
    "                return default\n",
    "            result = tree[key][query[key]]\n",
    "            \n",
    "            if isinstance(result,dict):\n",
    "                return predict_gini(query,result)\n",
    "\n",
    "            else:\n",
    "                return result\n",
    "            \n",
    "def test_gini(data,tree):\n",
    "    #Create new query instances by simply removing the target feature column from the original dataset and \n",
    "    #convert it to a dictionary\n",
    "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
    "    \n",
    "    #Create a empty DataFrame in whose columns the prediction of the tree are stored\n",
    "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
    "    predicted.sort_index(inplace=True)\n",
    "    \n",
    "    #Calculate the prediction accuracy\n",
    "    for i in range(len(data)):\n",
    "        predicted.loc[i,\"predicted\"] = predict_gini(queries[i],tree) \n",
    "        \n",
    "\n",
    "    columns = ['income']\n",
    "    testdf=pd.DataFrame(data[\"income\"], columns=columns)\n",
    "    accuracy = (np.sum(predicted[\"predicted\"] == testdf[\"income\"])/len(data)*100)\n",
    "    accuracyComparison_holdout.append(accuracy)\n",
    "    accuracy_giniIndex.append(accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Holdout Method</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout_gini(dataset):\n",
    "    training_data,testing_data = train_test_split(dataset,test_size=0.2, random_state=100)\n",
    "    training_data_gini = training_data.reset_index(drop=True)\n",
    "    testing_data_gini = testing_data.reset_index(drop=True)\n",
    "    tree = C45(training_data_gini,training_data_gini,training_data_gini.columns[:-1])\n",
    "    return test_gini(testing_data_gini,tree)\n",
    "resultGini = holdout_gini(dataUncleaned)\n",
    "our_error_C45 = 100 - resultGini\n",
    "resultGini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Bagging Ensemble Method</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bagging\n",
    "def bagging_gini(dataset,n):\n",
    "    result=0\n",
    "    for i in range(n):\n",
    "        rnge =int(len(dataset)/n)\n",
    "        start = i*rnge\n",
    "        end= start+ rnge\n",
    "        dataset = dataset[start:end]\n",
    "        training_data,testing_data = train_test_split(dataset,test_size=0.2, random_state=n*100)\n",
    "        training_data= training_data.reset_index(drop=True)\n",
    "        testing_data= testing_data.reset_index(drop=True)\n",
    "        tree = C45(training_data,training_data,training_data.columns[:-1])\n",
    "        accuracy_for_each_bag = test(testing_data,tree)\n",
    "        result+=accuracy_for_each_bag\n",
    "        accuracy = result/n\n",
    "        accuracy_giniIndex.append(accuracy)\n",
    "    print(result/n)\n",
    "bagging_gini(dataUncleaned,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Which method gives the most accurate value for Tree Decision using gini index Classifier?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracyPlot_giniIndex(methodList, accuracyList):\n",
    "    fig = go.Figure(data=[go.Pie(labels=methodList, values=accuracyList)])\n",
    "    return fig.update_traces(hoverinfo='label+value', textinfo='percent', marker=dict(colors= ['indianred', 'darkred']))\n",
    "accuracyPlot_giniIndex(usedMethods_giniIndex, accuracy_giniIndex).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span>Naïve Bayes</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_naiveBayes = []\n",
    "usedMethods_naiveBayes = ['Holdout Method', 'Cross-Validation Method', 'Bagging Ensemble Method', 'Boosting Ensemble Method']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Holdout Method</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_holdout(dataset):\n",
    "    x = dataset.drop('income', axis=1).values \n",
    "    y = dataset['income'].values\n",
    "    X_train_hold, X_test_hold, Y_train_hold, Y_test_hold = train_test_split(x, y, test_size=0.2, random_state=100)\n",
    "    model_name = 'Naive Bayes Classifier'\n",
    "    nb_model = GaussianNB()\n",
    "    nb_model.fit(X_train_hold,Y_train_hold)\n",
    "    y_pred = nb_model.predict(X_test_hold) \n",
    "    accuracy = metrics.accuracy_score(Y_test_hold, y_pred)*100\n",
    "    accuracyComparison_holdout.append(accuracy)\n",
    "    accuracy_naiveBayes.append(accuracy)\n",
    "    print(\"Accuracy by Naïve Bayes via hold_out method:\", accuracy,\"%\")\n",
    "    return accuracy\n",
    "\n",
    "resultnb = nb_holdout(data)\n",
    "our_error_NB = 100 - resultnb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Cross-Validation Method</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_crossvalidation(dataset):\n",
    "    x = dataset.drop('income', axis=1).values \n",
    "    y = dataset['income'].values\n",
    "    cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    model_name = 'Naive Bayes Classifier'\n",
    "    nb_model = GaussianNB()\n",
    "    scores = cross_val_score(nb_model, x, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    # report performance\n",
    "    accuracy = (np.mean(scores))*100\n",
    "    accuracy_naiveBayes.append(accuracy)\n",
    "    print(\"Accuracy by Naïve Bayes via crossvalidation method: \" , (np.mean(scores))*100, '%')\n",
    "nb_crossvalidation(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Bagging Ensemble Method</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_bagging(dataset, n):\n",
    "    result =0\n",
    "    for i in range (n):\n",
    "        rnge =int(len(dataset)/n)\n",
    "        start = i*rnge\n",
    "        end= start+ rnge\n",
    "        dataset = dataset[start:end]\n",
    "        x = dataset.drop('income', axis=1).values \n",
    "        y = dataset['income'].values\n",
    "        X_train_hold, X_test_hold, Y_train_hold, Y_test_hold = train_test_split(x, y, test_size=0.2, random_state=n*100)\n",
    "        model_name = 'Naive Bayes Classifier'\n",
    "        nb_model = GaussianNB()\n",
    "        nb_model.fit(X_train_hold,Y_train_hold)\n",
    "        y_pred = nb_model.predict(X_test_hold)   \n",
    "        result +=metrics.accuracy_score(Y_test_hold, y_pred)\n",
    "        accuracy = (result/n)*100\n",
    "        accuracy_naiveBayes.append(accuracy)\n",
    "    print(\"Accuracy by Naïve Bayes via bagging method:\",(result/n)*100,'%')\n",
    "nb_bagging(data,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Boosting Ensemble Method</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_boosting(dataset):\n",
    "    x = data.drop('income', axis=1).values \n",
    "    y = data['income'].values\n",
    "    model_name = 'Naive Bayes Classifier'\n",
    "    nb_model = GaussianNB()\n",
    "    AdaBoost = AdaBoostClassifier(base_estimator= nb_model,n_estimators=400,learning_rate=1, algorithm='SAMME')\n",
    "    AdaBoost.fit(x,y)\n",
    "    prediction = AdaBoost.score(x,y)\n",
    "    accuracy = prediction*100\n",
    "    accuracy_naiveBayes.append(accuracy)\n",
    "    print('\"Accuracy by  Naïve Bayes via boosting method:\" ',prediction*100,'%')\n",
    "nb_boosting(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Which method gives the most accurate value for Naïve Bayes Classifier?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracyPlot_naiveBayes(methodList, accuracyList):\n",
    "    fig = go.Figure(data=[go.Pie(labels=methodList, values=accuracyList)])\n",
    "    return fig.update_traces(hoverinfo='label+value', textinfo='percent', marker=dict(colors= ['aliceblue', 'royalblue', 'darkblue', 'lightskyblue']))\n",
    "accuracyPlot_naiveBayes(usedMethods_naiveBayes, accuracy_naiveBayes).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span>Artificial neural networks with 1 hidden layer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_ann1 = []\n",
    "usedMethods_ann1 = ['Holdout Method', 'Cross-Validation Method', 'Bagging Ensemble Method']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Holdout Method</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann1_holdout(dataset):\n",
    "    x = dataset.drop('income', axis=1).values \n",
    "    y = dataset['income'].values\n",
    "    X_train_hold, X_test_hold, Y_train_hold, Y_test_hold = train_test_split(x, y, test_size=0.2, random_state=100)\n",
    "    model = MLPClassifier(hidden_layer_sizes=(10), max_iter=1000) #1 hidden layer with 10 hidden units\n",
    "    model.fit(X_train_hold, Y_train_hold)   #trainig the model\n",
    "    y_pred = model.predict(X_test_hold)   #prediction\n",
    "    accuracy = metrics.accuracy_score(Y_test_hold, y_pred)*100\n",
    "    accuracyComparison_holdout.append(accuracy)\n",
    "    accuracy_ann1.append(accuracy)\n",
    "    print(\"Accuracy by ANN with 1 layer via hold_out method:\", accuracy,\"%\")\n",
    "ann1_holdout(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Cross-Validation Method</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann1_crossvalidation(dataset):\n",
    "    x = dataset.drop('income', axis=1).values \n",
    "    y = dataset['income'].values\n",
    "    cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(10), max_iter=1000)\n",
    "    scores = cross_val_score(mlp, x, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    accuracy = (np.mean(scores))*100\n",
    "    accuracy_ann1.append(accuracy)\n",
    "    # report performance\n",
    "    print(\"Accuracy by neural network via cross_validation method (1 layer):\", (np.mean(scores))*100,'%')\n",
    "ann1_crossvalidation(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Bagging Ensemble Method</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann1_bagging(dataset, n):\n",
    "    result =0\n",
    "    for i in range (n):\n",
    "        rnge =int(len(dataset)/n)\n",
    "        start = i*rnge\n",
    "        end= start+ rnge\n",
    "        dataset = dataset[start:end]\n",
    "        x = dataset.drop('income', axis=1).values \n",
    "        y = dataset['income'].values\n",
    "        X_train_hold, X_test_hold, Y_train_hold, Y_test_hold = train_test_split(x, y, test_size=0.2, random_state=n*100)\n",
    "        model = MLPClassifier(hidden_layer_sizes=(10), max_iter=50 )\n",
    "        model.fit(X_train_hold,Y_train_hold)\n",
    "        y_pred = model.predict(X_test_hold)   \n",
    "        result +=metrics.accuracy_score(Y_test_hold, y_pred)\n",
    "        accuracy = (result/n)*100\n",
    "        accuracy_ann1.append(accuracy)\n",
    "    print(\"Accuracy by ANN with 1 layer via bagging method:\",(result/n)*100,'%')\n",
    "ann1_bagging(data,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Which method gives the most accurate value for Artificial neural networks with 1 hidden layer Classifier?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracyPlot_ann1(methodList, accuracyList):\n",
    "    fig = go.Figure(data=[go.Pie(labels=methodList, values=accuracyList)])\n",
    "    return fig.update_traces(hoverinfo='label+value', textinfo='percent', marker=dict(colors= ['lightgoldenrodyellow', 'wheat', 'yellow']))\n",
    "accuracyPlot_ann1(usedMethods_ann1, accuracy_ann1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span>Artificial neural networks with 2 hidden layers</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_ann2 = []\n",
    "usedMethods_ann2 = ['Holdout Method', 'Cross-Validation Method', 'Bagging Ensemble Method']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Holdout Method</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann2_holdout(dataset):\n",
    "    x = dataset.drop('income', axis=1).values \n",
    "    y = dataset['income'].values\n",
    "    X_train_hold, X_test_hold, Y_train_hold, Y_test_hold = train_test_split(x, y, test_size=0.2, random_state=100)\n",
    "    model = MLPClassifier(hidden_layer_sizes=(10,10), max_iter=1000) #2 hidden layers with 10 hidden units\n",
    "    model.fit(X_train_hold, Y_train_hold)   #trainig the model\n",
    "    y_pred = model.predict(X_test_hold)   #prediction\n",
    "    accuracy = metrics.accuracy_score(Y_test_hold, y_pred)*100\n",
    "    accuracyComparison_holdout.append(accuracy)\n",
    "    accuracy_ann2.append(accuracy)\n",
    "    print(\"Accuracy by ANN with 2 layers via hold_out method:\", accuracy,\"%\")\n",
    "ann2_holdout(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Cross-Validation Method</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann2_crossvalidation(dataset):\n",
    "    x = dataset.drop('income', axis=1).values \n",
    "    y = dataset['income'].values\n",
    "    cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(10), max_iter=1000)\n",
    "    scores = cross_val_score(mlp, x, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    # report performance\n",
    "    accuracy=(np.mean(scores))*100\n",
    "    accuracy_ann2.append(accuracy)\n",
    "    print(\"Accuracy by neural network via cross_validation method (2layers):\", (np.mean(scores))*100,'%')\n",
    "ann2_crossvalidation(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Bagging Ensemble Method</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann2_bagging(dataset, n):\n",
    "    result =0\n",
    "    for i in range (n):\n",
    "        rnge =int(len(dataset)/n)\n",
    "        start = i*rnge\n",
    "        end= start+ rnge\n",
    "        dataset = dataset[start:end]\n",
    "        x = dataset.drop('income', axis=1).values \n",
    "        y = dataset['income'].values\n",
    "        X_train_hold, X_test_hold, Y_train_hold, Y_test_hold = train_test_split(x, y, test_size=0.2, random_state=n*100)\n",
    "        model = MLPClassifier(hidden_layer_sizes=(10,10), max_iter=1000)\n",
    "        model.fit(X_train_hold,Y_train_hold)\n",
    "        y_pred = model.predict(X_test_hold)   \n",
    "        result +=metrics.accuracy_score(Y_test_hold, y_pred)\n",
    "        accuracy=(result/n)*100\n",
    "        accuracy_ann2.append(accuracy)\n",
    "    print(\"Accuracy by ANN with 1 layer via bagging method:\",(result/n)*100,'%')\n",
    "ann2_bagging(data,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span>Which method gives the most accurate value for Artificial neural networks with 2 hidden layers Classifier?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracyPlot_ann2(methodList, accuracyList):\n",
    "    fig = go.Figure(data=[go.Pie(labels=methodList, values=accuracyList)])\n",
    "    return fig.update_traces(hoverinfo='label+value', textinfo='percent', marker=dict(colors= ['lightslategrey', 'darkslategrey', 'lightgrey']))\n",
    "accuracyPlot_ann2(usedMethods_ann1, accuracy_ann1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span>Support vector machines</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_svm = []\n",
    "usedMethods_svm = ['Holdout Method', 'Cross-Validation Method', 'Bagging Ensemble Method', 'Boosting Ensemble Method']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Holdout Method</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_holdout(dataset):\n",
    "    x = dataset.drop('income', axis=1).values \n",
    "    y = dataset['income'].values\n",
    "    X_train_hold, X_test_hold, Y_train_hold, Y_test_hold = train_test_split(x, y, test_size=0.2, random_state=100)\n",
    "    clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "    clf.fit(X_train_hold, Y_train_hold)   #trainig the model\n",
    "    y_pred = clf.predict(X_test_hold)   #prediction\n",
    "    accuracy = metrics.accuracy_score(Y_test_hold, y_pred)*100\n",
    "    accuracyComparison_holdout.append(accuracy)\n",
    "    accuracy_svm.append(accuracy)\n",
    "    print(\"Accuracy by SVM via hold_out method:\", accuracy,\"%\")\n",
    "svm_holdout(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Cross-Validation Method</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_crossvalidation(dataset):\n",
    "    x = dataset.drop('income', axis=1).values \n",
    "    y = dataset['income'].values\n",
    "    cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "    scores = cross_val_score(clf, x, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    # report performance\n",
    "    accuracy = (np.mean(scores))*100\n",
    "    accuracy_svm.append(accuracy)\n",
    "    print(\"Accuracy by SVM via crossvalidation method: \" , (np.mean(scores))*100, '%')\n",
    "svm_crossvalidation(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Bagging Ensemble Method</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_bagging(dataset, n):\n",
    "    result =0\n",
    "    for i in range (n):\n",
    "        rnge =int(len(dataset)/n)\n",
    "        start = i*rnge\n",
    "        end= start+ rnge\n",
    "        dataset = dataset[start:end]\n",
    "        x = dataset.drop('income', axis=1).values \n",
    "        y = dataset['income'].values\n",
    "        X_train_hold, X_test_hold, Y_train_hold, Y_test_hold = train_test_split(x, y, test_size=0.2, random_state=n*100)\n",
    "        clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "        clf.fit(X_train_hold, Y_train_hold)   #trainig the model\n",
    "        y_pred = clf.predict(X_test_hold)   #prediction\n",
    "        result +=metrics.accuracy_score(Y_test_hold, y_pred)\n",
    "        accuracy = (result/n)*100\n",
    "        accuracy_svm.append(accuracy)\n",
    "    print(\"Accuracy by SVM via bagging method:\",(result/n)*100,'%')\n",
    "svm_bagging(data,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Boosting Ensemble Method</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = data[:10]\n",
    "def svm_boosting(dataset):\n",
    "    x = data.drop('income', axis=1).values \n",
    "    y = data['income'].values\n",
    "    model = svm.SVC(kernel='linear') # Linear Kernel\n",
    "    AdaBoost = AdaBoostClassifier(base_estimator= model,n_estimators=400,learning_rate=1, algorithm='SAMME')\n",
    "    AdaBoost.fit(x,y)\n",
    "    prediction = AdaBoost.score(x,y)\n",
    "    accuracy = prediction*100\n",
    "    accuracy_svm.append(accuracy)\n",
    "    print('\"Accuracy by SVM via boosting method:\" ',prediction*100,'%')\n",
    "svm_boosting(dataset2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>Which method gives the most accurate value for Support vector machines Classifier?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracyPlot_svm(methodList, accuracyList):\n",
    "    fig = go.Figure(data=[go.Pie(labels=methodList, values=accuracyList)])\n",
    "    return fig.update_traces(hoverinfo='label+value', textinfo='percent', marker=dict(colors= ['peachpuff', 'papayawhip', 'orange', 'orangered']))\n",
    "accuracyPlot_svm(usedMethods_svm, accuracy_svm).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#900C3F;\">4. Comparing the performance of classification models</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span>Comparison of accuracy among all classifiers used with Holdout Method</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Pie(labels=classifiersList, values=accuracyComparison_holdout)])\n",
    "fig = fig.update_traces(hoverinfo='label+value', textinfo='percent', marker=dict(colors= ['aliceblue', 'lightskyblue', 'royalblue', 'mediumblue', 'cornflowerblue', 'darkblue']))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span>Comparison of error accuracy between our results and Ronny-Barry results given from the website for ID3, C45 and Naïve Bayes algorithm</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing between error accuracies we got after training and testing, and the error accuracies done by\n",
    "#Ronny Kohavi and Barry Becker as given from the website (https://archive.ics.uci.edu/ml/datasets/census+income)\n",
    "\n",
    "#ID3: Decision tree by gain value\n",
    "#our_error_ID3 = \n",
    "Ronny_Barry_ID3 = 15.64\n",
    "\n",
    "#C4.5: Decision tree by gini index value\n",
    "#our_error_C45 = \n",
    "Ronny_Barry_C45 = 15.54\n",
    "\n",
    "#NB: Naive bayes\n",
    "#our_error_NB = \n",
    "Ronny_Barry_NB = 16.12\n",
    "list = ['ID3 Error', 'C45 Error', 'Naïve Bayes']\n",
    "our_error = [our_error_ID3, our_error_C45, our_error_NB]\n",
    "Ronny_Barry_error = [Ronny_Barry_ID3, Ronny_Barry_C45,Ronny_Barry_NB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Our Errors', x=list, y=our_error),\n",
    "    go.Bar(name='Ronny-Barry Errors', x=list, y=Ronny_Barry_error)\n",
    "])\n",
    "fig = fig.update_layout(barmode='group')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The Census Income data is preprocessed, splitted into testing & training sets by several methods such as Holdout method and Bagging ensemble method. Then, 6 models where some of them are constructed with ready models and other is manually implemented on the data in order to predict whether a person makes over 50K a year or not.\n",
    "\n",
    "\n",
    "Thank you! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References: \n",
    "\n",
    "- https://archive.ics.uci.edu/ml/datasets/Census+Income\n",
    "\n",
    "- http://myweb.sabanciuniv.edu/rdehkharghani/files/2016/02/The-Morgan-Kaufmann-Series-in-Data-Management-Systems-Jiawei-Han-Micheline-Kamber-Jian-Pei-Data-Mining.-Concepts-and-Techniques-3rd-Edition-Morgan-Kaufmann-2011.pdf\n",
    "\n",
    "- https://scikit-learn.org/stable/\n",
    "\n",
    "- https://plotly.com/python/basic-charts/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
