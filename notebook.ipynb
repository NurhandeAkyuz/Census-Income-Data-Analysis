{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#900C3F;\">Census Income Data Analysis</span>\n",
    "\n",
    "### <span style=\"color:#5E5C5B;\">In this notebook, using census income data, we construct various classification models, and implement them above on the data, and analyze the performance of each classification model.</span> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#900C3F;\">Steps taken:</span> \n",
    "1. **Setting up the environment.**\n",
    "2. **Preparing Data for Analysis.**\n",
    "3. **Implementation and Model evaluation.**\n",
    "4. **Comparing the performance of classification models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#900C3F;\">1. Setting up the environment</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing requried libraries**\n",
    "\n",
    "Mainly used:\n",
    "- **Apache spark** in this project, so we import the some libraries required, **pyspark** for starting a **spark session.** \n",
    "- **pandas** for data analysis and manipulation.\n",
    "- **sklearn** for statistical modeling including classification.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a **spark session** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#900C3F;\">2. Preparing Data for Analysis</span>\n",
    "\n",
    "**Preparing** dataset taken from http://archive.ics.uci.edu/ml/index.php for the analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset for **Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the file with spark session's method read.csv, converting the csv file to a dataframe\n",
    "def readCSVtoDF(csvData):\n",
    "    return spark.read.csv(csvData)\n",
    "\n",
    "rawData = 'Data/adult.data'\n",
    "\n",
    "CensusDF = readCSVtoDF(rawData)\n",
    "\n",
    "AttributeList = CensusDF.columns\n",
    "\n",
    "Attributes = {'_c0':'age', '_c1':'workclass', '_c2':'fnlwg', '_c3':'education', '_c4':'education_number', \n",
    "              '_c5':'marital_status', '_c6':'occupation', '_c7':'relationship', '_c8':'race', '_c9':'sex', \n",
    "              '_c10':'capital_gain', '_c11':'capital_loss', '_c12':'hours_per_week', '_c13':'native_country', \n",
    "              '_c14':'income'}\n",
    "\n",
    "#changing the name of attributes with original attribute names\n",
    "for col in AttributeList:\n",
    "    CensusDF = CensusDF.withColumnRenamed(col, Attributes[col])\n",
    "    \n",
    "#converting dataframe to Pandas Dataframe\n",
    "def convertDFtoPandas(df):\n",
    "    return df.select('*').toPandas()\n",
    "data = convertDFtoPandas(CensusDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset for **Naïve Bayes, Artificial Neural Networks and Support Vector Machines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### prepocessing data for categorical features #######\n",
    "#workclass\n",
    "encoder_workclass = OrdinalEncoder()\n",
    "data.workclass = encoder_workclass.fit_transform(data.workclass.values.reshape(-1, 1))\n",
    "data.loc[data['workclass'] == 8, 'workclass'] = data['workclass'].mode()\n",
    "\n",
    "#marital_status\n",
    "encoder_marital_status = OrdinalEncoder()\n",
    "data.marital_status = encoder_marital_status.fit_transform(data.marital_status.values.reshape(-1, 1))\n",
    "data.loc[data['marital_status'] == 7, 'marital_status'] = data['marital_status'].mode()\n",
    "\n",
    "#occupation\n",
    "data.loc[data['occupation'].isnull(), 'occupation'] = data['occupation'].mode()\n",
    "encoder_occupation= OrdinalEncoder()\n",
    "data.occupation= encoder_occupation.fit_transform(data.occupation.values.reshape(-1,1))\n",
    "data.loc[data['occupation']== 14, 'occupation'] = data['occupation'].mode()\n",
    "\n",
    "#relationship\n",
    "encoder_relationship = OrdinalEncoder()\n",
    "data.relationship = encoder_relationship.fit_transform(data.relationship.values.reshape(-1, 1))\n",
    "#no null in this one\n",
    "\n",
    "#race\n",
    "encoder_race = OrdinalEncoder()\n",
    "data.race = encoder_race.fit_transform(data.race.values.reshape(-1, 1))\n",
    "#no null in this one\n",
    "\n",
    "#sex\n",
    "encoder_sex = OrdinalEncoder()\n",
    "data.sex = encoder_sex.fit_transform(data.sex.values.reshape(-1, 1))\n",
    "#no null in this one\n",
    "\n",
    "\n",
    "#native_country\n",
    "encoder_native_country= OrdinalEncoder()\n",
    "data.native_country= encoder_native_country.fit_transform(data.native_country.values.reshape(-1,1))\n",
    "data.loc[data['native_country']== 41, 'native_country'] = data['native_country'].mode()\n",
    "\n",
    "#income\n",
    "encoder_income = OrdinalEncoder()\n",
    "data.income= encoder_income.fit_transform(data.income.values.reshape(-1, 1))\n",
    "#no null in this one\n",
    "\n",
    "#education--- should be changed later\n",
    "\n",
    "encoder_education= OrdinalEncoder()\n",
    "data.education= encoder_education.fit_transform(data.education.values.reshape(-1, 1))\n",
    "#no null in this one\n",
    "\n",
    "\n",
    "age_scaler = MinMaxScaler(feature_range=(0,1))     #ages to be between o and 1 \n",
    "data.age =age_scaler.fit_transform(data.age.values.reshape(-1, 1))\n",
    "\n",
    "fnlwg_scaler = MinMaxScaler(feature_range=(0,1))    \n",
    "data.fnlwg =fnlwg_scaler.fit_transform(data.fnlwg.values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "education_number_scaler = MinMaxScaler(feature_range=(0,1))    \n",
    "data.education_number=education_number_scaler.fit_transform(data.education_number.values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "diff1 =int(max(data['capital_gain']))-int(min(data['capital_gain']))\n",
    "data['capital_gain'] = data['capital_gain'].apply(lambda x: int(x)/diff1)\n",
    "\n",
    "\n",
    "diff2 =int(max(data['capital_loss']))-int(min(data['capital_loss']))\n",
    "data['capital_loss'] = data['capital_loss'].apply(lambda x: int(x)/diff2)\n",
    "\n",
    "\n",
    "diff3 =int(max(data['hours_per_week']))-int(min(data['hours_per_week']))\n",
    "data['hours_per_week'] = data['hours_per_week'].apply(lambda x: int(x)/diff3)\n",
    "\n",
    "\n",
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "data = clean_dataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#900C3F;\">3. Implementation and Model evaluation</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_holdout={'Decision tree using gain ratio': 0, 'Decision tree using gini index': 0, 'Naïve Bayes': 0, \n",
    "          'Artificial neural networks with 1 hidden layer' : 0, 'Artificial neural networks with 2 hidden layers': 0, \n",
    "          'Support vector machines' : 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span>Decision tree using gain ratio</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the entropy of each column\n",
    "def entropy(col): \n",
    "    values,counts = np.unique(col,return_counts = True) \n",
    "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(values))])\n",
    "    return entropy\n",
    "\n",
    "#calculating the gain\n",
    "def Gain(dataframe,split_attribute,target_name=\"income\"):   #it will always be income\n",
    "  \n",
    "    total_entropy = entropy(dataframe[target_name])    #Calculating the entropy of target name\n",
    "    Weighted_Entropy =  entropy(dataframe[split_attribute])\n",
    "    \n",
    "    #Calculate the information gain\n",
    "    Information_Gain = total_entropy - Weighted_Entropy\n",
    "    return Information_Gain  \n",
    "\n",
    "#ID3 algorithm for decision tree using gain \n",
    "\n",
    "def ID3(data,originaldata,features,target_attribute_name=\"income\",parent_node = None):\n",
    "    \n",
    "    #if all the target column values are same, return that value! no need to grow the tree\n",
    "    \n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:            \n",
    "    \n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    "      \n",
    "   \n",
    "    \n",
    "    elif len(features) ==0:               # when the feature space is empty\n",
    "        return parent_node\n",
    "    \n",
    "    \n",
    "    #growing the tree\n",
    "    \n",
    "    else:\n",
    "      \n",
    "        #the default value for the parent node is that value that appears the most in that specific feature.\n",
    "        parent_node = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n",
    "        \n",
    "        #calculating Gain for each feature\n",
    "        item_values = [Gain(data,feature,target_attribute_name) for feature in features]\n",
    "        \n",
    "        #choosing the highest feature\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        best_feature = features[best_feature_index] #to be assigned as the root \n",
    "        \n",
    "        #Tree structure\n",
    "        \n",
    "        tree = {best_feature:{}}\n",
    "        \n",
    "        \n",
    "        # remove the best feature\n",
    "        features = [i for i in features if i != best_feature]\n",
    "        \n",
    "        #Grow a branch under the root node for each possible value of the root node feature\n",
    "        \n",
    "        for value in np.unique(data[best_feature]):\n",
    "            value = value\n",
    "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n",
    "            sub_data = data.where(data[best_feature] == value).dropna()\n",
    "            \n",
    "            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
    "            subtree = ID3(sub_data,originaldata,features,target_attribute_name,parent_node)\n",
    "            \n",
    "            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
    "            tree[best_feature][value] = subtree\n",
    "            \n",
    "        return(tree)    \n",
    "    \n",
    "# for future predictions a query of specified \n",
    "#features will be given and prediction function will go through the tree to find the result    \n",
    "def predict(query,tree,default = 1): \n",
    "    \n",
    "    for key in list(query.keys()): \n",
    "        if key in list(tree.keys()):\n",
    "            try:\n",
    "                result = tree[key][query[key]] \n",
    "            except:\n",
    "                return default\n",
    "            result = tree[key][query[key]]\n",
    "            \n",
    "            if isinstance(result,dict):\n",
    "                return predict(query,result)\n",
    "\n",
    "            else:\n",
    "                return result\n",
    "\n",
    "def test(data,tree):\n",
    "    #Create new query instances by simply removing the target feature column from the original dataset and \n",
    "    #convert it to a dictionary\n",
    "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
    "    \n",
    "    #Create a empty DataFrame in whose columns the prediction of the tree are stored\n",
    "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
    "    predicted.sort_index(inplace=True)\n",
    "    \n",
    "    #Calculate the prediction accuracy\n",
    "    for i in range(len(data)):\n",
    "        predicted.loc[i,\"predicted\"] = predict(queries[i],tree) \n",
    "        \n",
    "\n",
    "    columns = ['income']\n",
    "    testdf=pd.DataFrame(data[\"income\"], columns=columns)    \n",
    "    accuracy = (np.sum(predicted[\"predicted\"] == testdf[\"income\"])/len(data)*100)\n",
    "    accuracy_holdout['Decision tree using gain ratio'] = accuracy\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Holdout Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#holdout\n",
    "def holdout_entropy(dataset):\n",
    "    training_data,testing_data = train_test_split(dataset,test_size=0.2, random_state=100)\n",
    "    training_data_hold = training_data.reset_index(drop=True)\n",
    "    testing_data_hold = testing_data.reset_index(drop=True)\n",
    "    tree = ID3(training_data_hold,training_data_hold,training_data_hold.columns[:-1])\n",
    "    return test(testing_data_hold,tree)\n",
    "holdout_entropy(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bagging Ensemble Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bagging\n",
    "def bagging_entropy(dataset,n):\n",
    "    result=0\n",
    "    for i in range(n):\n",
    "        rnge =int(len(dataset)/n)\n",
    "        start = i*rnge\n",
    "        end= start+ rnge\n",
    "        dataset = dataset[start:end]\n",
    "        training_data,testing_data = train_test_split(dataset,test_size=0.2, random_state=n*100)\n",
    "        training_data= training_data.reset_index(drop=True)\n",
    "        testing_data= testing_data.reset_index(drop=True)\n",
    "        tree = ID3(training_data,training_data,training_data.columns[:-1])\n",
    "        accuracy_for_each_bag = test(testing_data,tree)\n",
    "        result+=accuracy_for_each_bag\n",
    "    print(result/n)\n",
    "bagging_entropy(data,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span>Decision tree using gini index</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## decision tree by gini \n",
    "\n",
    "def gini(col): \n",
    "    values,counts = np.unique(col,return_counts = True) \n",
    "    gini = np.sum([(1-(counts[i]/np.sum(counts))*(counts[i]/np.sum(counts))) for i in range(len(values))])\n",
    "    return gini\n",
    "\n",
    "#calculating the gini difference\n",
    "def ginidiff(dataframe,split_attribute,target_name=\"income\"):   #it will always be income\n",
    "  \n",
    "    total_gini = gini(dataframe[target_name])    #Calculating the entropy of target name\n",
    "    Weighted_gini =  gini(dataframe[split_attribute])\n",
    "    \n",
    "    #Calculate the information gain\n",
    "    gini_diff = total_gini - Weighted_gini\n",
    "    return gini_diff  \n",
    "\n",
    "#c45 algorithm for decision tree using gain \n",
    "def C45(data,originaldata,features,target_attribute_name=\"income\",parent_node = None):\n",
    "    \n",
    "    #if all the target column values are same, return that value! no need to grow the tree\n",
    "    \n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:            \n",
    "    \n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    "      \n",
    "   \n",
    "    \n",
    "    elif len(features) ==0:               # when the feature space is empty\n",
    "        return parent_node\n",
    "    \n",
    "    \n",
    "    #growing the tree\n",
    "    \n",
    "    else:\n",
    "      \n",
    "        #the default value for the parent node is that value that appears the most in that specific feature.\n",
    "        parent_node = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n",
    "        \n",
    "        #calculating Gain for each feature\n",
    "        item_values = [ginidiff(data,feature,target_attribute_name) for feature in features]\n",
    "        \n",
    "        #choosing the highest feature\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        best_feature = features[best_feature_index] #to be assigned as the root \n",
    "        \n",
    "        #Tree structure\n",
    "        \n",
    "        tree = {best_feature:{}}\n",
    "        \n",
    "        \n",
    "        # remove the best feature\n",
    "        features = [i for i in features if i != best_feature]\n",
    "        \n",
    "        #Grow a branch under the root node for each possible value of the root node feature\n",
    "        \n",
    "        for value in np.unique(data[best_feature]):\n",
    "            value = value\n",
    "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n",
    "            sub_data = data.where(data[best_feature] == value).dropna()\n",
    "            \n",
    "            #Call the C45 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
    "            subtree = C45(sub_data,originaldata,features,target_attribute_name,parent_node)\n",
    "            \n",
    "            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
    "            tree[best_feature][value] = subtree\n",
    "            \n",
    "        return(tree)    \n",
    "\n",
    "# for future predictions a query of specified \n",
    "#features will be given and prediction function will go through the tree to find the result    \n",
    "def predict_gini(query,tree,default = 1): \n",
    "    \n",
    "    for key in list(query.keys()): \n",
    "        if key in list(tree.keys()):\n",
    "            try:\n",
    "                result = tree[key][query[key]] \n",
    "            except:\n",
    "                return default\n",
    "            result = tree[key][query[key]]\n",
    "            \n",
    "            if isinstance(result,dict):\n",
    "                return predict_gini(query,result)\n",
    "\n",
    "            else:\n",
    "                return result\n",
    "            \n",
    "def test_gini(data,tree):\n",
    "    #Create new query instances by simply removing the target feature column from the original dataset and \n",
    "    #convert it to a dictionary\n",
    "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
    "    \n",
    "    #Create a empty DataFrame in whose columns the prediction of the tree are stored\n",
    "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
    "    predicted.sort_index(inplace=True)\n",
    "    \n",
    "    #Calculate the prediction accuracy\n",
    "    for i in range(len(data)):\n",
    "        predicted.loc[i,\"predicted\"] = predict_gini(queries[i],tree) \n",
    "        \n",
    "\n",
    "    columns = ['income']\n",
    "    testdf=pd.DataFrame(data[\"income\"], columns=columns)\n",
    "    accuracy = (np.sum(predicted[\"predicted\"] == testdf[\"income\"])/len(data)*100)\n",
    "    accuracy_holdout['Decision tree using gini index'] = accuracy\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Holdout Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout_gini(dataset):\n",
    "    training_data,testing_data = train_test_split(dataset,test_size=0.2, random_state=100)\n",
    "    training_data_gini = training_data.reset_index(drop=True)\n",
    "    testing_data_gini = testing_data.reset_index(drop=True)\n",
    "    tree = C45(training_data_gini,training_data_gini,training_data_gini.columns[:-1])\n",
    "    return test_gini(testing_data_gini,tree)\n",
    "holdout_gini(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bagging Ensemble Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bagging\n",
    "def bagging_gini(dataset,n):\n",
    "    result=0\n",
    "    for i in range(n):\n",
    "        rnge =int(len(dataset)/n)\n",
    "        start = i*rnge\n",
    "        end= start+ rnge\n",
    "        dataset = dataset[start:end]\n",
    "        training_data,testing_data = train_test_split(dataset,test_size=0.2, random_state=n*100)\n",
    "        training_data= training_data.reset_index(drop=True)\n",
    "        testing_data= testing_data.reset_index(drop=True)\n",
    "        tree = C45(training_data,training_data,training_data.columns[:-1])\n",
    "        accuracy_for_each_bag = test(testing_data,tree)\n",
    "        result+=accuracy_for_each_bag\n",
    "    print(result/n)\n",
    "bagging_gini(data,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span>Naïve Bayes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Holdout Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_holdout(dataset):\n",
    "    x = dataset.drop('income', axis=1).values \n",
    "    y = dataset['income'].values\n",
    "    X_train_hold, X_test_hold, Y_train_hold, Y_test_hold = train_test_split(x, y, test_size=0.2, random_state=100)\n",
    "    model_name = 'Naive Bayes Classifier'\n",
    "    nb_model = GaussianNB()\n",
    "    nb_model.fit(X_train_hold,Y_train_hold)\n",
    "    y_pred = nb_model.predict(X_test_hold) \n",
    "    accuracy = metrics.accuracy_score(Y_test_hold, y_pred)*100\n",
    "    accuracy_holdout['Naïve Bayes'] = accuracy\n",
    "    print(\"Accuracy by Naïve Bayes via hold_out method:\", accuracy,\"%\")\n",
    "nb_holdout(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-Validation Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_crossvalidation(dataset):\n",
    "    x = dataset.drop('income', axis=1).values \n",
    "    y = dataset['income'].values\n",
    "    cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    model_name = 'Naive Bayes Classifier'\n",
    "    nb_model = GaussianNB()\n",
    "    scores = cross_val_score(nb_model, x, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    # report performance\n",
    "    print(\"Accuracy by Naïve Bayes via crossvalidation method: \" , (np.mean(scores))*100, '%')\n",
    "nb_crossvalidation(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bagging Ensemble Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_bagging(dataset, n):\n",
    "    result =0\n",
    "    for i in range (n):\n",
    "        rnge =int(len(dataset)/n)\n",
    "        start = i*rnge\n",
    "        end= start+ rnge\n",
    "        dataset = dataset[start:end]\n",
    "        x = dataset.drop('income', axis=1).values \n",
    "        y = dataset['income'].values\n",
    "        X_train_hold, X_test_hold, Y_train_hold, Y_test_hold = train_test_split(x, y, test_size=0.2, random_state=n*100)\n",
    "        model_name = 'Naive Bayes Classifier'\n",
    "        nb_model = GaussianNB()\n",
    "        nb_model.fit(X_train_hold,Y_train_hold)\n",
    "        y_pred = nb_model.predict(X_test_hold)   \n",
    "        result +=metrics.accuracy_score(Y_test_hold, y_pred)\n",
    "    print(\"Accuracy by Naïve Bayes via bagging method:\",(result/n)*100,'%')\n",
    "nb_bagging(data,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boosting Ensemble Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_boosting(dataset):\n",
    "    x = data.drop('income', axis=1).values \n",
    "    y = data['income'].values\n",
    "    model_name = 'Naive Bayes Classifier'\n",
    "    nb_model = GaussianNB()\n",
    "    AdaBoost = AdaBoostClassifier(base_estimator= nb_model,n_estimators=400,learning_rate=1, algorithm='SAMME')\n",
    "    AdaBoost.fit(x,y)\n",
    "    prediction = AdaBoost.score(x,y)\n",
    "    print('\"Accuracy by  Naïve Bayes via boosting method:\" ',prediction*100,'%')\n",
    "nb_boosting(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span>Artificial neural networks with 1 hidden layer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Holdout Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann1_holdout(dataset):\n",
    "    x = dataset.drop('income', axis=1).values \n",
    "    y = dataset['income'].values\n",
    "    X_train_hold, X_test_hold, Y_train_hold, Y_test_hold = train_test_split(x, y, test_size=0.2, random_state=100)\n",
    "    model = MLPClassifier(hidden_layer_sizes=(14,)) #1 hidden layer with 14 hidden units\n",
    "    model.fit(X_train_hold, Y_train_hold)   #trainig the model\n",
    "    y_pred = model.predict(X_test_hold)   #prediction\n",
    "    accuracy = metrics.accuracy_score(Y_test_hold, y_pred)*100\n",
    "    accuracy_holdout['Artificial neural networks with 1 hidden layer'] = accuracy\n",
    "    print(\"Accuracy by ANN with 1 layer via hold_out method:\", accuracy,\"%\")\n",
    "ann1_holdout(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-Validation Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bagging Ensemble Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann1_bagging(dataset, n):\n",
    "    result =0\n",
    "    for i in range (n):\n",
    "        rnge =int(len(dataset)/n)\n",
    "        start = i*rnge\n",
    "        end= start+ rnge\n",
    "        dataset = dataset[start:end]\n",
    "        x = dataset.drop('income', axis=1).values \n",
    "        y = dataset['income'].values\n",
    "        X_train_hold, X_test_hold, Y_train_hold, Y_test_hold = train_test_split(x, y, test_size=0.2, random_state=n*100)\n",
    "        model = MLPClassifier(hidden_layer_sizes=(14,))\n",
    "        model.fit(X_train_hold,Y_train_hold)\n",
    "        y_pred = model.predict(X_test_hold)   \n",
    "        result +=metrics.accuracy_score(Y_test_hold, y_pred)\n",
    "    print(\"Accuracy by ANN with 1 layer via bagging method:\",(result/n)*100,'%')\n",
    "ann1_bagging(data,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boosting Ensemble Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span>Artificial neural networks with 2 hidden layers</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Holdout Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann2_holdout(dataset):\n",
    "    x = dataset.drop('income', axis=1).values \n",
    "    y = dataset['income'].values\n",
    "    X_train_hold, X_test_hold, Y_train_hold, Y_test_hold = train_test_split(x, y, test_size=0.2, random_state=100)\n",
    "    model = MLPClassifier(hidden_layer_sizes=(14,14)) #2 hidden layers with 14 hidden units\n",
    "    model.fit(X_train_hold, Y_train_hold)   #trainig the model\n",
    "    y_pred = model.predict(X_test_hold)   #prediction\n",
    "    accuracy = metrics.accuracy_score(Y_test_hold, y_pred)*100\n",
    "    accuracy_holdout['Artificial neural networks with 1 hidden layer'] = accuracy\n",
    "    print(\"Accuracy by ANN with 2 layers via hold_out method:\", accuracy,\"%\")\n",
    "ann2_holdout(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-Validation Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bagging Ensemble Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann2_bagging(dataset, n):\n",
    "    result =0\n",
    "    for i in range (n):\n",
    "        rnge =int(len(dataset)/n)\n",
    "        start = i*rnge\n",
    "        end= start+ rnge\n",
    "        dataset = dataset[start:end]\n",
    "        x = dataset.drop('income', axis=1).values \n",
    "        y = dataset['income'].values\n",
    "        X_train_hold, X_test_hold, Y_train_hold, Y_test_hold = train_test_split(x, y, test_size=0.2, random_state=n*100)\n",
    "        model = MLPClassifier(hidden_layer_sizes=(14,14))\n",
    "        model.fit(X_train_hold,Y_train_hold)\n",
    "        y_pred = model.predict(X_test_hold)   \n",
    "        result +=metrics.accuracy_score(Y_test_hold, y_pred)\n",
    "    print(\"Accuracy by ANN with 1 layer via bagging method:\",(result/n)*100,'%')\n",
    "ann2_bagging(data,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boosting Ensemble Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span>Support vector machines</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Holdout Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_holdout(dataset):\n",
    "    x = dataset.drop('income', axis=1).values \n",
    "    y = dataset['income'].values\n",
    "    X_train_hold, X_test_hold, Y_train_hold, Y_test_hold = train_test_split(x, y, test_size=0.2, random_state=100)\n",
    "    clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "    clf.fit(X_train_hold, Y_train_hold)   #trainig the model\n",
    "    y_pred = clf.predict(X_test_hold)   #prediction\n",
    "    accuracy = metrics.accuracy_score(Y_test_hold, y_pred)*100\n",
    "    accuracy_holdout['Support vector machines'] = accuracy\n",
    "    print(\"Accuracy by SVM via hold_out method:\", accuracy,\"%\")\n",
    "svm_holdout(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-Validation Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_crossvalidation(dataset):\n",
    "    x = dataset.drop('income', axis=1).values \n",
    "    y = dataset['income'].values\n",
    "    cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "    scores = cross_val_score(clf, x, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    # report performance\n",
    "    print(\"Accuracy by SVM via crossvalidation method: \" , (np.mean(scores))*100, '%')\n",
    "svm_crossvalidation(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bagging Ensemble Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_bagging(dataset, n):\n",
    "    result =0\n",
    "    for i in range (n):\n",
    "        rnge =int(len(dataset)/n)\n",
    "        start = i*rnge\n",
    "        end= start+ rnge\n",
    "        dataset = dataset[start:end]\n",
    "        x = dataset.drop('income', axis=1).values \n",
    "        y = dataset['income'].values\n",
    "        X_train_hold, X_test_hold, Y_train_hold, Y_test_hold = train_test_split(x, y, test_size=0.2, random_state=n*100)\n",
    "        clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "        clf.fit(X_train_hold, Y_train_hold)   #trainig the model\n",
    "        y_pred = clf.predict(X_test_hold)   #prediction\n",
    "        result +=metrics.accuracy_score(Y_test_hold, y_pred)\n",
    "    print(\"Accuracy by SVM via bagging method:\",(result/n)*100,'%')\n",
    "svm_bagging(data,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boosting Ensemble Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_boosting(dataset):\n",
    "    x = data.drop('income', axis=1).values \n",
    "    y = data['income'].values\n",
    "    model = svm.SVC(kernel='linear') # Linear Kernel\n",
    "    AdaBoost = AdaBoostClassifier(base_estimator= model,n_estimators=400,learning_rate=1, algorithm='SAMME')\n",
    "    AdaBoost.fit(x,y)\n",
    "    prediction = AdaBoost.score(x,y)\n",
    "    print('\"Accuracy by SVM via boosting method:\" ',prediction*100,'%')\n",
    "svm_boosting(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in accuracy_holdout.keys():\n",
    "    labels.append(i)\n",
    "sum=0\n",
    "new_values = []\n",
    "for i in accuracy_holdout.values():\n",
    "    sum += i\n",
    "for i in accuracy_holdout.values():\n",
    "    new_values.append((sum/sum)/i*10)\n",
    "print(new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Pie(labels=labels, values=new_values)])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "pop_a = mpatches.Patch(color='#BB6B5A', label=' <=50K')\n",
    "pop_b = mpatches.Patch(color='#E5E88B', label=' >50K')\n",
    "\n",
    "def colormap(risk_list):\n",
    "    cols=[]\n",
    "    for l in risk_list:\n",
    "        if l==0:\n",
    "            cols.append('#BB6B5A')\n",
    "        elif l==1:\n",
    "            cols.append('#E5E88B')\n",
    "    return cols\n",
    "\n",
    "def two_d_compare(y_test,y_pred,model_name):\n",
    "    #y_pred = label_encoder.fit_transform(y_pred)\n",
    "    #y_test = label_encoder.fit_transform(y_test)\n",
    "    area = (12 * np.random.rand(40))**2 \n",
    "    plt.subplots(ncols=2, figsize=(10,4))\n",
    "    plt.suptitle('Actual vs Predicted data : ' +model_name + '. Accuracy : %.2f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.scatter(X_test['Education'], X_test['Sex'], alpha=0.8, c=colormap(y_test))\n",
    "    plt.title('Actual')\n",
    "    plt.legend(handles=[pop_a,pop_b])\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.scatter(X_test['Education'], X_test['Sex'],alpha=0.8, c=colormap(y_pred))\n",
    "    plt.title('Predicted')\n",
    "    plt.legend(handles=[pop_a,pop_b])\n",
    "\n",
    "    plt.show()\n",
    "two_d_compare(y_test,y_pred_nb,model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#900C3F;\">4. Comparing the performance of classification models</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
